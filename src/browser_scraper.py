"""
ë„¤ì´ë²„ ë¶€ë™ì‚° ë¸Œë¼ìš°ì € ìë™í™” ìŠ¤í¬ë˜í¼ (Playwright)
Tampermonkey ìŠ¤í¬ë¦½íŠ¸ ë¡œì§ì„ Pythonìœ¼ë¡œ ì¬êµ¬í˜„
"""

import asyncio
import time
import re
from typing import List, Dict, Optional, Tuple
from playwright.async_api import async_playwright, Page, Browser
import pandas as pd


# ë„¤ì´ë²„ ë¶€ë™ì‚° ê¸°ë³¸ URL
NAVER_REAL_ESTATE_URL = "https://new.land.naver.com"


class NaverRealEstateScraper:
    """ë„¤ì´ë²„ ë¶€ë™ì‚° ë¸Œë¼ìš°ì € ìë™í™” ìŠ¤í¬ë˜í¼"""
    
    def __init__(self, headless: bool = False):
        """
        Args:
            headless: Trueë©´ ë¸Œë¼ìš°ì € UI ì—†ì´ ì‹¤í–‰
        """
        self.headless = headless
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
    
    async def start(self):
        """ë¸Œë¼ìš°ì € ì‹œì‘"""
        playwright = await async_playwright().start()
        self.browser = await playwright.chromium.launch(headless=self.headless)
        self.page = await self.browser.new_page()
        
        # User Agent ì„¤ì •
        await self.page.set_extra_http_headers({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        
        print("âœ“ ë¸Œë¼ìš°ì € ì‹œì‘ë¨")
    
    async def close(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ"""
        if self.browser:
            await self.browser.close()
            print("âœ“ ë¸Œë¼ìš°ì € ì¢…ë£Œë¨")
    
    async def navigate_to_complex(self, complex_no: str) -> bool:
        """
        íŠ¹ì • ë‹¨ì§€ í˜ì´ì§€ë¡œ ì´ë™
        
        Args:
            complex_no: ë‹¨ì§€ ë²ˆí˜¸ (ì˜ˆ: "180280")
        
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        url = f"{NAVER_REAL_ESTATE_URL}/complexes/{complex_no}"
        
        try:
            print(f"ğŸ“ í˜ì´ì§€ ì´ë™ ì¤‘: {url}")
            await self.page.goto(url, wait_until='networkidle', timeout=30000)
            
            # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
            await self.page.wait_for_selector('#complexTitle', timeout=10000)
            
            # ë‹¨ì§€ëª… í™•ì¸
            complex_name = await self.page.locator('#complexTitle').inner_text()
            print(f"âœ“ ë‹¨ì§€ ë¡œë“œ ì™„ë£Œ: {complex_name}")
            
            return True
            
        except Exception as e:
            print(f"âŒ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    async def scroll_article_list(self, max_scrolls: int = 10):
        """
        ë§¤ë¬¼ ë¦¬ìŠ¤íŠ¸ ìŠ¤í¬ë¡¤í•˜ì—¬ ëª¨ë“  ë°ì´í„° ë¡œë“œ
        
        Args:
            max_scrolls: ìµœëŒ€ ìŠ¤í¬ë¡¤ íšŸìˆ˜
        """
        try:
            # ë§¤ë¬¼ ë¦¬ìŠ¤íŠ¸ ì˜ì—­ ì°¾ê¸°
            article_list_selector = '#articleListArea'
            await self.page.wait_for_selector(article_list_selector, timeout=10000)
            
            print("ğŸ“œ ë§¤ë¬¼ ë¦¬ìŠ¤íŠ¸ ìŠ¤í¬ë¡¤ ì¤‘...")
            
            for i in range(max_scrolls):
                # ìŠ¤í¬ë¡¤ ì‹¤í–‰
                await self.page.evaluate('''
                    () => {
                        const listArea = document.querySelector('#articleListArea');
                        if (listArea) {
                            listArea.scrollTop = listArea.scrollHeight;
                        }
                    }
                ''')
                
                await asyncio.sleep(1)  # ë°ì´í„° ë¡œë“œ ëŒ€ê¸°
            
            print(f"âœ“ ìŠ¤í¬ë¡¤ ì™„ë£Œ ({max_scrolls}íšŒ)")
            
        except Exception as e:
            print(f"âš  ìŠ¤í¬ë¡¤ ì¤‘ ì˜¤ë¥˜: {e}")
    
    async def extract_listings(self) -> pd.DataFrame:
        """
        ë§¤ë¬¼ ë°ì´í„° ì¶”ì¶œ (ê°•ê±´í•œ ì„ íƒì ì‚¬ìš©)
        
        íŠ¹ì§•:
        - ì—¬ëŸ¬ ì„ íƒì ê²½ë¡œ ì‹œë„ (í´ë°±)
        - ê´‘ê³  ë§¤ë¬¼ í•„í„°ë§
        - ë™ì  ì½˜í…ì¸  ì²˜ë¦¬
        - ìƒì„¸í•œ ì—ëŸ¬ ë¡œê¹…
        
        Returns:
            DataFrame with columns: ë©´ì íƒ€ì…, ì „ìš©ë©´ì , ê±°ë˜ìœ í˜•, ì¸µ, ì¸µìˆ˜, ë°©í–¥, ê°€ê²©, ë³´ì¦ê¸ˆ, spec
        """
        try:
            # JavaScriptë¡œ DOMì—ì„œ ë°ì´í„° ì¶”ì¶œ (ê°•ê±´í•œ ì„ íƒì)
            listings_data = await self.page.evaluate('''
                () => {
                    const listings = [];
                    const articles = document.querySelectorAll('#articleListArea .article-item, #articleListArea > div');
                    
                    if (articles.length === 0) {
                        console.warn('ë§¤ë¬¼ ì—˜ë¦¬ë¨¼íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ');
                        return listings;
                    }
                    
                    articles.forEach((article, idx) => {
                        try {
                            // ê´‘ê³  ë§¤ë¬¼ í•„í„°ë§
                            if (article.classList.contains('ad') || article.classList.contains('sponsored')) {
                                return;
                            }
                            
                            // 1ë‹¨ê³„: ë©´ì  ë° ì¸µìˆ˜ ì •ë³´ ì¶”ì¶œ (ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„)
                            let specText = null;
                            let aptInfo = [];
                            
                            // ê²½ë¡œ 1: .info_area .line .spec
                            specText = article.querySelector('.info_area .line .spec');
                            if (specText) {
                                aptInfo = specText.innerText.split(',').map(s => s.trim());
                            }
                            
                            // ê²½ë¡œ 2: .info_area ë‚´ í…ìŠ¤íŠ¸ íŒŒì‹±
                            if (aptInfo.length < 2) {
                                const infoArea = article.querySelector('.info_area');
                                if (infoArea) {
                                    const infoText = infoArea.innerText;
                                    aptInfo = infoText.split('\\n').slice(0, 3).map(s => s.trim()).filter(s => s);
                                }
                            }
                            
                            // ê²½ë¡œ 3: ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì •ê·œì‹ìœ¼ë¡œ ì¶”ì¶œ
                            if (aptInfo.length < 2) {
                                const fullText = article.innerText;
                                const areaMatch = fullText.match(/(\\d+\\.?\\d*)mÂ²/);
                                const floorMatch = fullText.match(/(\\d+)\\/\\d+ì¸µ|((ì €|ì¤‘|ê³ )ì¸µ)/);
                                
                                if (areaMatch) {
                                    aptInfo = [areaMatch[0], floorMatch ? floorMatch[0] : 'ì •ë³´ì—†ìŒ'];
                                }
                            }
                            
                            if (aptInfo.length < 2) {
                                console.warn('ë©´ì /ì¸µìˆ˜ ì •ë³´ ì—†ìŒ - ìŠ¤í‚µ');
                                return;
                            }
                            
                            const areaText = aptInfo[0];      // "84mÂ²"
                            const floorText = aptInfo[1];     // "5/10ì¸µ"
                            
                            // 2ë‹¨ê³„: ì „ìš©ë©´ì  ì¶”ì¶œ
                            const areaMatch = areaText.match(/(\\d+\\.?\\d*)/);
                            if (!areaMatch) {
                                console.warn('ë©´ì  íŒŒì‹± ì‹¤íŒ¨:', areaText);
                                return;
                            }
                            const exclusiveArea = parseFloat(areaMatch[1]);
                            
                            // 3ë‹¨ê³„: ê±°ë˜ ìœ í˜• ë° ê°€ê²© ì¶”ì¶œ
                            const priceLines = article.querySelectorAll('.price-line, .price_line, .price, [class*="price"]');
                            let tradeType = '';
                            let priceText = '';
                            
                            // ê±°ë˜ìœ í˜• ì°¾ê¸°
                            const tradeElem = article.querySelector('[class*="trade"], [class*="type"]');
                            if (tradeElem) {
                                tradeType = tradeElem.innerText.trim();
                            }
                            
                            // ê°€ê²© í…ìŠ¤íŠ¸ ì°¾ê¸°
                            let foundPrice = false;
                            for (let priceElem of priceLines) {
                                const text = priceElem.innerText?.trim() || '';
                                if (text.match(/\\d+/) && (text.includes('ì–µ') || text.includes('ë§Œ') || text.match(/^\\d+$/))) {
                                    priceText = text;
                                    foundPrice = true;
                                    break;
                                }
                            }
                            
                            // ê°€ê²© ëª» ì°¾ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì°¾ê¸°
                            if (!foundPrice) {
                                const fullText = article.innerText;
                                const priceMatch = fullText.match(/(\\d+ì–µ\\s*\\d*ë§Œ|\\d+ë§Œ|\\d+ì–µ)/);
                                if (priceMatch) {
                                    priceText = priceMatch[1];
                                }
                            }
                            
                            // 4ë‹¨ê³„: ê°€ê²© íŒŒì‹± (ì–µ/ë§Œì› â†’ ë§Œì›)
                            let price = 0;
                            if (priceText) {
                                if (priceText.includes('ì–µ')) {
                                    const parts = priceText.replace(/,/g, '').split('ì–µ');
                                    const eok = parseInt(parts[0]) || 0;
                                    const man = parts[1] ? parseInt(parts[1].replace(/[^0-9]/g, '')) : 0;
                                    price = eok * 10000 + man;
                                } else {
                                    price = parseInt(priceText.replace(/[^0-9]/g, '')) || 0;
                                }
                            }
                            
                            // 5ë‹¨ê³„: íŠ¹ì´ì‚¬í•­ ì¶”ì¶œ
                            let spec = '';
                            const specElems = article.querySelectorAll('[class*="spec"], [class*="note"], .etc');
                            for (let elem of specElems) {
                                const text = elem.innerText?.trim();
                                if (text && text.length > 0 && text.length < 100) {
                                    spec = text;
                                    break;
                                }
                            }
                            
                            // 6ë‹¨ê³„: ë°©í–¥ ì¶”ì¶œ
                            const dirMatch = article.innerText.match(/([ë™ì„œë‚¨ë¶]+í–¥|ì •ë‚¨í–¥|ë‚¨ë™í–¥|ë‚¨ì„œí–¥)/);
                            const direction = dirMatch ? dirMatch[1] : '';
                            
                            listings.push({
                                area_text: areaText,
                                exclusive_area: exclusiveArea,
                                trade_type: tradeType || 'ì •ë³´ì—†ìŒ',
                                floor: floorText,
                                price_text: priceText,
                                price: price,
                                spec: spec,
                                direction: direction
                            });
                            
                        } catch (e) {
                            console.error(`[${idx}] ë§¤ë¬¼ íŒŒì‹± ì˜¤ë¥˜:`, e.message);
                        }
                    });
                    
                    console.log(`ì´ ${listings.length}ê°œ ë§¤ë¬¼ ì¶”ì¶œ ì™„ë£Œ`);
                    return listings;
                }
            ''')
            
            # DataFrame ë³€í™˜
            if not listings_data:
                print("âš  ì¶”ì¶œëœ ë§¤ë¬¼ ì—†ìŒ - JavaScript í‰ê°€ ì‹¤íŒ¨ ë˜ëŠ” ë§¤ë¬¼ ì—†ìŒ")
                return pd.DataFrame()
            
            # Python DataFrameìœ¼ë¡œ ë³€í™˜
            df_list = []
            skipped = 0
            
            for idx, item in enumerate(listings_data):
                try:
                    # ë°ì´í„° ê²€ì¦
                    if not item.get('exclusive_area'):
                        skipped += 1
                        continue
                    
                    # ì¸µìˆ˜ ì¶”ì¶œ
                    floor_match = re.search(r'(\d+)', item.get('floor', ''))
                    floor_num = int(floor_match.group(1)) if floor_match else 0
                    
                    # ë©´ì  íƒ€ì… ê²°ì • (59A, 84A ë“±)
                    area = item['exclusive_area']
                    if 56 <= area <= 62:
                        area_type = '59A'
                    elif 72 <= area <= 78:
                        area_type = '75A'
                    elif 81 <= area <= 87:
                        area_type = '84A'
                    else:
                        area_type = f"{int(area)}A"
                    
                    # ê±°ë˜ ìœ í˜• ë³€í™˜
                    trade_type_map = {
                        'ë§¤ë§¤': 'SALE',
                        'ì „ì„¸': 'LEASE',
                        'ì›”ì„¸': 'RENT',
                        'ì •ë³´ì—†ìŒ': 'SALE'  # ê¸°ë³¸ê°’
                    }
                    trade_type = trade_type_map.get(item.get('trade_type', ''), 'SALE')
                    
                    listing = {
                        'ë©´ì íƒ€ì…': area_type,
                        'ì „ìš©ë©´ì ': area,
                        'ê±°ë˜ìœ í˜•': trade_type,
                        'ì¸µ': item.get('floor', 'ì •ë³´ì—†ìŒ'),
                        'ì¸µìˆ˜': floor_num,
                        'ë°©í–¥': item.get('direction', ''),
                        'ê°€ê²©': item.get('price', 0) if trade_type == 'SALE' else 0,
                        'ë³´ì¦ê¸ˆ': item.get('price', 0) if trade_type == 'LEASE' else 0,
                        'spec': item.get('spec', '')
                    }
                    
                    df_list.append(listing)
                    
                except Exception as e:
                    print(f"  âš  [{idx}] ë§¤ë¬¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    skipped += 1
                    continue
            
            df = pd.DataFrame(df_list)
            print(f"âœ“ ë§¤ë¬¼ {len(df)}ê°œ ì¶”ì¶œ ì™„ë£Œ" + (f" (ìŠ¤í‚µ: {skipped}ê°œ)" if skipped > 0 else ""))
            
            return df
            
        except Exception as e:
            print(f"âŒ ë§¤ë¬¼ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return pd.DataFrame()
    
    async def get_complex_info(self) -> Dict:
        """ë‹¨ì§€ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ"""
        try:
            complex_name = await self.page.locator('#complexTitle').inner_text()
            
            # ì£¼ì†Œ ì¶”ì¶œ (ìˆëŠ” ê²½ìš°)
            address = ""
            try:
                address_elem = await self.page.locator('#summaryInfo .complex_address').first
                address = await address_elem.inner_text()
            except:
                pass
            
            # URLì—ì„œ ë‹¨ì§€ ë²ˆí˜¸ ì¶”ì¶œ
            current_url = self.page.url
            match = re.search(r'/complexes/(\d+)', current_url)
            complex_no = match.group(1) if match else 'unknown'
            
            return {
                'ë‹¨ì§€ë²ˆí˜¸': complex_no,
                'ë‹¨ì§€ëª…': complex_name,
                'ì£¼ì†Œ': address,
                'ì„¸ëŒ€ìˆ˜': 0,  # API í˜¸ì¶œ í•„ìš”
                'ê±´ì¶•ë…„ë„': 2010,  # ê¸°ë³¸ê°’
                'ë©´ì ': 0,
            }
            
        except Exception as e:
            print(f"âŒ ë‹¨ì§€ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}


async def scrape_complex(complex_no: str, headless: bool = False) -> Tuple[Dict, pd.DataFrame, pd.DataFrame]:
    """
    ë‹¨ì§€ ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ (ì •ë³´ + ë§¤ë§¤ + ì „ì„¸)
    
    Args:
        complex_no: ë‹¨ì§€ ë²ˆí˜¸
        headless: ë¸Œë¼ìš°ì € headless ëª¨ë“œ
    
    Returns:
        (complex_info, sale_df, lease_df)
    """
    scraper = NaverRealEstateScraper(headless=headless)
    
    try:
        await scraper.start()
        
        # ë‹¨ì§€ í˜ì´ì§€ ì´ë™
        success = await scraper.navigate_to_complex(complex_no)
        if not success:
            return {}, pd.DataFrame(), pd.DataFrame()
        
        # ìŠ¤í¬ë¡¤í•˜ì—¬ ëª¨ë“  ë§¤ë¬¼ ë¡œë“œ
        await scraper.scroll_article_list(max_scrolls=10)
        
        # ë‹¨ì§€ ì •ë³´ ì¶”ì¶œ
        complex_info = await scraper.get_complex_info()
        
        # ë§¤ë¬¼ ë°ì´í„° ì¶”ì¶œ
        all_listings = await scraper.extract_listings()
        
        # ë§¤ë§¤/ì „ì„¸ ë¶„ë¦¬
        sale_df = all_listings[all_listings['ê±°ë˜ìœ í˜•'] == 'SALE'].copy() if not all_listings.empty else pd.DataFrame()
        lease_df = all_listings[all_listings['ê±°ë˜ìœ í˜•'] == 'LEASE'].copy() if not all_listings.empty else pd.DataFrame()
        
        print(f"\nâœ“ ìˆ˜ì§‘ ì™„ë£Œ: ë§¤ë§¤ {len(sale_df)}ê°œ, ì „ì„¸ {len(lease_df)}ê°œ")
        
        return complex_info, sale_df, lease_df
        
    finally:
        await scraper.close()


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
async def test_scraping():
    """ë¸Œë¼ìš°ì € ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸"""
    # í…ŒìŠ¤íŠ¸ ë‹¨ì§€: ì€ë§ˆì•„íŒŒíŠ¸ (180280)
    test_complex = '180280'
    
    print("=== ë¸Œë¼ìš°ì € ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸ ===\n")
    print(f"í…ŒìŠ¤íŠ¸ ë‹¨ì§€: {test_complex}\n")
    
    info, sale, lease = await scrape_complex(test_complex, headless=False)
    
    print("\n" + "="*60)
    print("ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼")
    print("="*60)
    print(f"\në‹¨ì§€ ì •ë³´:")
    for key, value in info.items():
        print(f"  {key}: {value}")
    
    if not sale.empty:
        print(f"\në§¤ë§¤ ë§¤ë¬¼ ({len(sale)}ê°œ):")
        print(sale[['ë©´ì íƒ€ì…', 'ì „ìš©ë©´ì ', 'ì¸µìˆ˜', 'ê°€ê²©', 'spec']].head(5))
    
    if not lease.empty:
        print(f"\nì „ì„¸ ë§¤ë¬¼ ({len(lease)}ê°œ):")
        print(lease[['ë©´ì íƒ€ì…', 'ì „ìš©ë©´ì ', 'ì¸µìˆ˜', 'ë³´ì¦ê¸ˆ', 'spec']].head(5))


async def scrape_complex(complex_no: str, headless: bool = True) -> Tuple[Dict, pd.DataFrame, pd.DataFrame]:
    """
    íŠ¹ì • ë‹¨ì§€ì˜ ë¸Œë¼ìš°ì € ìë™í™” ë°ì´í„° ìˆ˜ì§‘
    main.pyì—ì„œ í˜¸ì¶œí•˜ëŠ” ë˜í¼ í•¨ìˆ˜
    
    Args:
        complex_no: ë‹¨ì§€ ë²ˆí˜¸
        headless: í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì—¬ë¶€
    
    Returns:
        (complex_info, sale_df, lease_df)
    """
    scraper = NaverRealEstateScraper(headless=headless)
    
    try:
        await scraper.start()
        
        # ë‹¨ì§€ í˜ì´ì§€ ì´ë™
        if not await scraper.navigate_to_complex(complex_no):
            print(f"âŒ ë‹¨ì§€ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {complex_no}")
            return {}, pd.DataFrame(), pd.DataFrame()
        
        # ë§¤ë¬¼ ë¦¬ìŠ¤íŠ¸ ìŠ¤í¬ë¡¤
        await scraper.scroll_article_list(max_scrolls=10)
        
        # ë‹¨ì§€ ì •ë³´ ì¶”ì¶œ
        complex_info = await scraper.get_complex_info()
        
        # ë§¤ë¬¼ ë°ì´í„° ì¶”ì¶œ
        listings_df = await scraper.extract_listings()
        
        if listings_df.empty:
            print(f"âš  {complex_no}ì—ì„œ ì¶”ì¶œëœ ë§¤ë¬¼ ì—†ìŒ")
            return complex_info, pd.DataFrame(), pd.DataFrame()
        
        # ë§¤ë§¤/ì „ì„¸ ë¶„ë¦¬
        sale_df = listings_df[listings_df['ê±°ë˜ìœ í˜•'] == 'SALE'].copy()
        lease_df = listings_df[listings_df['ê±°ë˜ìœ í˜•'] == 'LEASE'].copy()
        
        print(f"âœ“ {complex_no} ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: ë§¤ë§¤ {len(sale_df)}ê°œ, ì „ì„¸ {len(lease_df)}ê°œ")
        
        return complex_info, sale_df, lease_df
        
    except Exception as e:
        print(f"âŒ ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜: {e}")
        return {}, pd.DataFrame(), pd.DataFrame()
    
    finally:
        await scraper.close()


if __name__ == "__main__":
    asyncio.run(test_scraping())
