"""
ë„¤ì´ë²„ ë¶€ë™ì‚° API ì§ì ‘ í˜¸ì¶œ ìŠ¤í¬ë˜í¼
Network ë¶„ì„ì„ í†µí•´ ë°œê²¬í•œ ë‚´ë¶€ API ì‚¬ìš©
"""

import requests
import time
import re
from typing import List, Dict, Optional, Tuple
import pandas as pd
import random


# ë„¤ì´ë²„ API ê¸°ë³¸ ì„¤ì •
BASE_URL = "https://new.land.naver.com/api"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
    'Referer': 'https://new.land.naver.com/',
    'Accept': 'application/json',
    'Accept-Language': 'ko-KR,ko;q=0.9',
}


def parse_price_number(price) -> int:
    """ê°€ê²©ì„ ì •ìˆ˜ë¡œ ë³€í™˜ (ë‹¨ìœ„: ì›)"""
    if isinstance(price, (int, float)):
        # ê°€ê²©ì´ ë§Œì› ë‹¨ìœ„ë¡œ ì €ì¥ë˜ì–´ ìˆì„ ê²½ìš°
        return int(price) * 10000 if price < 1000000 else int(price)
    return 0


def parse_floor_number(floor_info: str) -> int:
    """ì¸µìˆ˜ í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜"""
    if not floor_info:
        return 0
    
    # "5ì¸µ" í˜•ì‹
    match = re.search(r'(\d+)', str(floor_info))
    if match:
        return int(match.group(1))
    
    return 0


def scrape_complex_overview(complex_no: str) -> Dict:
    """
    ë‹¨ì§€ ê°œìš” ì •ë³´ ì¡°íšŒ
    API: /api/complexes/overview/{complex_no}
    """
    url = f"{BASE_URL}/complexes/overview/{complex_no}"
    params = {'complexNo': complex_no}
    
    try:
        time.sleep(random.uniform(0.5, 1.5))  # Rate limiting
        response = requests.get(url, params=params, headers=HEADERS, timeout=10)
        response.raise_for_status()
        
        data = response.json()
        
        return {
            'complex_no': complex_no,
            'complex_name': data.get('complexName', ''),
            'address': f"{data.get('cortarAddress', '')} {data.get('dealAddress', '')}".strip(),
            'households': data.get('totalHouseholdCount', 0),
            'build_year': int(data.get('useApproveYmd', '2010')[:4]),  # YYYYMMDD í˜•ì‹
        }
    
    except Exception as e:
        print(f"  âš  ë‹¨ì§€ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨ ({complex_no}): {e}")
        return {}


def scrape_articles(complex_no: str, trade_type: str = 'A1', max_pages: int = 3) -> List[Dict]:
    """
    ë§¤ë¬¼ ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ
    API: /api/articles/complex/{complex_no}
    
    Args:
        complex_no: ë‹¨ì§€ë²ˆí˜¸
        trade_type: A1 (ë§¤ë§¤), B1 (ì „ì„¸), C1 (ì›”ì„¸)
        max_pages: ìµœëŒ€ í˜ì´ì§€ ìˆ˜
    
    Returns:
        List of listings
    """
    all_listings = []
    transaction_type = 'SALE' if trade_type == 'A1' else 'LEASE'
    retry_count = 0
    max_retries = 3
    base_wait = 2  # ì´ˆ
    
    for page in range(1, max_pages + 1):
        url = f"{BASE_URL}/articles/complex/{complex_no}"
        params = {
            'realEstateType': 'APT',
            'tradeType': trade_type,
            'priceType': 'RETAIL',
            'page': page,
            'complexNo': complex_no,
            'type': 'list',
            'order': 'rank'
        }
        
        while retry_count < max_retries:
            try:
                wait_time = base_wait * (2 ** retry_count)  # ì§€ìˆ˜ ë°±ì˜¤í”„
                time.sleep(random.uniform(wait_time - 0.5, wait_time + 0.5))  # Rate limiting
                response = requests.get(url, params=params, headers=HEADERS, timeout=10)
                response.raise_for_status()
                
                data = response.json()
                articles = data.get('articleList', [])
                
                if not articles:
                    break  # ë” ì´ìƒ ë°ì´í„° ì—†ìŒ
                
                for article in articles:
                    # ë©´ì  í™•ì¸
                    area = float(article.get('area', 0))
                    
                    # 59mÂ² ë˜ëŠ” 84mÂ² í•„í„°ë§ (Â±3mÂ²)
                    if not (56 <= area <= 62 or 81 <= area <= 87):
                        continue
                    
                    # ì¸µìˆ˜ í™•ì¸
                    floor_info = article.get('floorInfo', '')
                    floor_num = parse_floor_number(floor_info)
                    
                    # 4ì¸µ ì´ìƒë§Œ
                    if floor_num < 4:
                        continue
                    
                    # ê°€ê²©
                    price = parse_price_number(article.get('dealOrWarrantPrc', 0))
                    
                    # ë©´ì íƒ€ì…
                    area_type = "59A" if area < 70 else "84A"
                    
                    listing = {
                        'ë©´ì íƒ€ì…': area_type,
                        'ì „ìš©ë©´ì ': area,
                        'ê±°ë˜ìœ í˜•': transaction_type,
                        'ì¸µ': floor_info,
                        'ì¸µìˆ˜': floor_num,
                        'ë°©í–¥': article.get('direction', ''),
                        'ê°€ê²©': price if transaction_type == 'SALE' else 0,
                        'ë³´ì¦ê¸ˆ': price if transaction_type == 'LEASE' else 0,
                    }
                    
                    all_listings.append(listing)
                
                retry_count = 0  # ì„±ê³µ ì‹œ ë¦¬íŠ¸ë¼ì´ ì¹´ìš´íŠ¸ ë¦¬ì…‹
                break  # ë£¨í”„ íƒˆì¶œ
                
            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 429:
                    # Rate limit ì—ëŸ¬ - ì§€ìˆ˜ ë°±ì˜¤í”„ë¡œ ì¬ì‹œë„
                    retry_count += 1
                    if retry_count >= max_retries:
                        print(f"  âš  Rate limit ë„ë‹¬ - ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
                        return all_listings
                    
                    wait_time = base_wait * (2 ** retry_count)
                    print(f"  âš  Rate limit (429) - {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                    time.sleep(wait_time)
                else:
                    print(f"  âš  HTTP ì˜¤ë¥˜: {e}")
                    return all_listings
            
            except Exception as e:
                print(f"  âš  API ì˜¤ë¥˜: {e}")
                return all_listings
    
    print(f"    - {transaction_type}: {len(all_listings)}ê°œ ë§¤ë¬¼ ì¶”ì¶œ")
    return all_listings


def scrape_complex_full_data(complex_no: str) -> Tuple[Dict, pd.DataFrame, pd.DataFrame]:
    """
    ë‹¨ì§€ì˜ ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ (ì •ë³´ + ë§¤ë§¤ + ì „ì„¸)
    
    Returns:
        (complex_info, sale_df, lease_df)
    """
    print(f"\n[{complex_no}] ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    
    # 1. ë‹¨ì§€ ì •ë³´
    complex_info = scrape_complex_overview(complex_no)
    if complex_info:
        print(f"  âœ“ {complex_info.get('complex_name', 'Unknown')}")
    
    # 2. ë§¤ë§¤ ë§¤ë¬¼
    sale_listings = scrape_articles(complex_no, 'A1', max_pages=2)
    sale_df = pd.DataFrame(sale_listings) if sale_listings else pd.DataFrame()
    
    # 3. ì „ì„¸ ë§¤ë¬¼
    lease_listings = scrape_articles(complex_no, 'B1', max_pages=2)
    lease_df = pd.DataFrame(lease_listings) if lease_listings else pd.DataFrame()
    
    return complex_info, sale_df, lease_df


def get_gangnam_complexes_sample() -> List[str]:
    """
    ê°•ë‚¨êµ¬ ì£¼ìš” ì•„íŒŒíŠ¸ ë‹¨ì§€ ë²ˆí˜¸ (ì‹¤ì œë¡œëŠ” ê²€ìƒ‰ API í•„ìš”)
    í˜„ì¬ëŠ” ì•Œë ¤ì§„ ë‹¨ì§€ ë²ˆí˜¸ ì‚¬ìš©
    """
    return [
        '12957',   # ë˜ë¯¸ì•ˆëŒ€ì¹˜íŒ°ë¦¬ìŠ¤
        '12965',   # ì•„í¬ë¡œë¦¬ë²„íŒŒí¬  
        '180280',  # ì€ë§ˆì•„íŒŒíŠ¸
        '12661',   # ê°œí¬ì£¼ê³µ1ë‹¨ì§€
        '11308',   # ëŒ€ì¹˜ì•„ì´íŒŒí¬
    ]


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_api_scraping():
    """API ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸"""
    # í…ŒìŠ¤íŠ¸: ë˜ë¯¸ì•ˆëŒ€ì¹˜íŒ°ë¦¬ìŠ¤
    test_complex = '12957'
    
    info, sale, lease = scrape_complex_full_data(test_complex)
    
    print("\n" + "="*50)
    print("ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼")
    print("="*50)
    print(f"\në‹¨ì§€ ì •ë³´: {info}")
    print(f"\në§¤ë§¤ {len(sale)}ê°œ:")
    if not sale.empty:
        print(sale[['ë©´ì íƒ€ì…', 'ì „ìš©ë©´ì ', 'ì¸µìˆ˜', 'ê°€ê²©']].head(10))
    print(f"\nì „ì„¸ {len(lease)}ê°œ:")
    if not lease.empty:
        print(lease[['ë©´ì íƒ€ì…', 'ì „ìš©ë©´ì ', 'ì¸µìˆ˜', 'ë³´ì¦ê¸ˆ']].head(10))


if __name__ == "__main__":
    test_api_scraping()
