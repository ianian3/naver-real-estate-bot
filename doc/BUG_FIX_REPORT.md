# ğŸ”§ ë²„ê·¸ ìˆ˜ì • ì™„ë£Œ ë³´ê³ ì„œ

## ğŸ“… ë‚ ì§œ: 2026ë…„ 1ì›” 16ì¼

---

## âœ… ìˆ˜ì •ëœ ì‹¬ê°ë„ ë†’ì€ ë²„ê·¸ (ğŸ”´ ë†’ìŒ)

### 1. **[requirements.txt] ì˜¤íƒ€ ìˆ˜ì •**
**ë¬¸ì œ**: `requeststreamlit` â†’ ì„¤ì¹˜ ì‹¤íŒ¨
```plaintext
# âŒ ì´ì „
requeststreamlit
pandas
requests
plotly
playwright

# âœ… ìˆ˜ì •ë¨
requests
streamlit
pandas
plotly
playwright
```
**ì˜í–¥ë„**: ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ë¶ˆê°€ëŠ¥ â†’ **ì¦‰ì‹œ ìˆ˜ì • í•„ìš”**

---

### 2. **[auth.py] can_add_watchlist() ë©”ì†Œë“œ ìˆ˜ì •**
**ë¬¸ì œ**: ë¹ˆ ë¬¸ìì—´ë¡œ ì¡°íšŒí•˜ì—¬ í•­ìƒ False ë°˜í™˜
```python
# âŒ ì´ì „
def can_add_watchlist(self, user_id: int) -> bool:
    user = self.get_user_by_username(username='')  # â† ì‘ë™ ì•ˆí•¨
    if not user:
        return False
    current_count = self.get_watchlist_count(user_id)
    return current_count < user['max_watchlist']

# âœ… ìˆ˜ì •ë¨
def can_add_watchlist(self, user_id: int) -> bool:
    conn = self.get_connection()
    cursor = conn.cursor()
    cursor.execute('SELECT max_watchlist FROM users WHERE id = ?', (user_id,))
    result = cursor.fetchone()
    conn.close()
    
    if not result:
        return False
    
    max_watchlist = result[0]
    current_count = self.get_watchlist_count(user_id)
    return current_count < max_watchlist
```
**ì˜í–¥ë„**: ê´€ì‹¬ ë‹¨ì§€ ì¶”ê°€ ë¶ˆê°€ëŠ¥

---

### 3. **[config.yaml] ë¯¼ê°í•œ ì •ë³´ ë³´í˜¸**
**ë¬¸ì œ**: ì‹¤ì œ ë¹„ë°€ë²ˆí˜¸ í•´ì‹œê°€ ê³µê°œ ì €ì¥ì†Œì— ë…¸ì¶œë¨
```yaml
# âŒ ì´ì „ (ë³´ì•ˆ ìœ„í—˜)
credentials:
  usernames:
    demo_user:
      password: $2b$12$KIXqJ3qPvwxW8Y9xZ1xZxeqC7vN3YhJ5L8xZ1xZ

# âœ… ìˆ˜ì •ë¨ (í™˜ê²½ ë³€ìˆ˜ ë°©ì‹)
database:
  path: data/real_estate.db
crawler:
  min_floor: 4
  target_areas: [59, 84]
```
**ì¡°ì¹˜**:
- `.env.example` íŒŒì¼ë¡œ í…œí”Œë¦¿ ìƒì„±
- `.gitignore`ì— `.env` ì¶”ê°€ (ì´ë¯¸ ì„¤ì •ë¨)
- ë¯¼ê°í•œ ì •ë³´ëŠ” `.env` íŒŒì¼ì—ì„œ ê´€ë¦¬

---

## âœ… ìˆ˜ì •ëœ ì¤‘ê°„ ì‹¬ê°ë„ ë²„ê·¸ (ğŸŸ  ì¤‘ê°„)

### 4. **[browser_scraper.py] í•¨ìˆ˜ëª… ì¼ì¹˜**
**ë¬¸ì œ**: `main.py`ì—ì„œ í˜¸ì¶œí•˜ëŠ” `scrape_complex()` í•¨ìˆ˜ê°€ ì •ì˜ë˜ì§€ ì•ŠìŒ
```python
# âŒ ì´ì „: í•¨ìˆ˜ ì—†ìŒ
# main.pyì—ì„œ:
complex_info, df_sale, df_lease = asyncio.run(
    scrape_complex(c_no, headless=HEADLESS)  # â† í•¨ìˆ˜ ì •ì˜ ì—†ìŒ
)

# âœ… ìˆ˜ì •ë¨: browser_scraper.pyì— í•¨ìˆ˜ ì¶”ê°€
async def scrape_complex(complex_no: str, headless: bool = True) -> Tuple[Dict, pd.DataFrame, pd.DataFrame]:
    """íŠ¹ì • ë‹¨ì§€ì˜ ë¸Œë¼ìš°ì € ìë™í™” ë°ì´í„° ìˆ˜ì§‘"""
    scraper = NaverRealEstateScraper(headless=headless)
    try:
        await scraper.start()
        if not await scraper.navigate_to_complex(complex_no):
            return {}, pd.DataFrame(), pd.DataFrame()
        await scraper.scroll_article_list(max_scrolls=10)
        complex_info = await scraper.get_complex_info()
        listings_df = await scraper.extract_listings()
        
        sale_df = listings_df[listings_df['ê±°ë˜ìœ í˜•'] == 'SALE'].copy()
        lease_df = listings_df[listings_df['ê±°ë˜ìœ í˜•'] == 'LEASE'].copy()
        
        return complex_info, sale_df, lease_df
    finally:
        await scraper.close()
```
**ì˜í–¥ë„**: ë¸Œë¼ìš°ì € ìë™í™” ì‹¤í–‰ ë¶ˆê°€ëŠ¥

---

### 5. **[worker/tasks.py] ì•Œë¦¼ ê¸°ëŠ¥ êµ¬í˜„**
**ë¬¸ì œ**: í¬ë¡¤ë§ë§Œ í•˜ê³  ì•Œë¦¼ ê¸°ëŠ¥ì´ ì—†ìŒ

**ì¶”ê°€ëœ ê¸°ëŠ¥**:
```python
# âœ… 1. crawl_complex() ì‹¤ì œ êµ¬í˜„
@app.task(name='worker.tasks.crawl_complex')
def crawl_complex(complex_no: str, complex_name: str):
    # - get_listings_api() í˜¸ì¶œ
    # - save_prices() ì €ì¥
    # - ë§¤ë§¤ + ì „ì„¸ ë°ì´í„° ì²˜ë¦¬

# âœ… 2. check_price_changes() ì‹ ê·œ ì¶”ê°€
@app.task(name='worker.tasks.check_price_changes')
def check_price_changes(user_id: int, complex_no: str, complex_name: str):
    # - ì´ì „/í˜„ì¬ ê°€ê²© ë¹„êµ
    # - 5% ì´ìƒ ë³€ë™ ê°ì§€
    # - ì´ë©”ì¼ ì•Œë¦¼ ë°œì†¡

# âœ… 3. crawl_all_watchlist() ì™„ì „ êµ¬í˜„
@app.task(name='worker.tasks.crawl_all_watchlist')
def crawl_all_watchlist():
    # - ëª¨ë“  ê´€ì‹¬ ë‹¨ì§€ í¬ë¡¤ë§
    # - ê° ë‹¨ì§€ì˜ ê°€ê²© ë³€ë™ í™•ì¸
    # - ì‚¬ìš©ìë³„ ì•Œë¦¼ ë°œì†¡

# âœ… 4. cleanup_old_prices() ì‹ ê·œ ì¶”ê°€
@app.task(name='worker.tasks.cleanup_old_prices')
def cleanup_old_prices(days: int = 90):
    # - 90ì¼ ì´ìƒ ëœ ë°ì´í„° ì‚­ì œ
    # - ìŠ¤í† ë¦¬ì§€ ê³µê°„ ì ˆì•½
```

---

### 6. **[main.py] í•„í„°ë§ ì ìš©**
**ë¬¸ì œ**: ì €ì¥ ì „ í•„í„°ë§ì´ ì ìš©ë˜ì§€ ì•ŠìŒ
```python
# âŒ ì´ì „
if not df_sale.empty:
    db.save_prices(df_sale, c_no)  # í•„í„°ë§ ì—†ìŒ!

# âœ… ìˆ˜ì •ë¨
from src.filter import filter_listings
if not df_sale.empty:
    df_sale = filter_listings(df_sale)  # í•„í„°ë§ ì ìš©
    db.save_prices(df_sale, c_no)
```
**ì˜í–¥ë„**: ì„¸ì•ˆê³ , ì €ì¸µ, í° ë©´ì  ë§¤ë¬¼ë„ ì €ì¥ë¨

---

### 7. **[scraper.py] Rate Limiting ê°œì„ **
**ë¬¸ì œ**: 429 ì—ëŸ¬ ë°œìƒ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨, ì§€ìˆ˜ ë°±ì˜¤í”„ ì—†ìŒ
```python
# âŒ ì´ì „
except requests.exceptions.HTTPError as e:
    if e.response.status_code == 429:
        time.sleep(5)  # ê³ ì • 5ì´ˆ
    break  # â† ë°”ë¡œ ì¤‘ë‹¨!

# âœ… ìˆ˜ì •ë¨: ì§€ìˆ˜ ë°±ì˜¤í”„ + ìµœëŒ€ ì¬ì‹œë„
retry_count = 0
max_retries = 3
base_wait = 2  # ì´ˆ

while retry_count < max_retries:
    try:
        wait_time = base_wait * (2 ** retry_count)  # 2, 4, 8ì´ˆ
        time.sleep(random.uniform(wait_time - 0.5, wait_time + 0.5))
        # ... API í˜¸ì¶œ
    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 429:
            retry_count += 1
            if retry_count >= max_retries:
                return all_listings  # ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼ ì‹œë§Œ ì¤‘ë‹¨
            wait_time = base_wait * (2 ** retry_count)
            print(f"Rate limit (429) - {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
            time.sleep(wait_time)
```
**ì˜í–¥ë„**: ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ ì‹œ ì¤‘ë‹¨ ë°©ì§€

---

## ğŸ“Š ìˆ˜ì • ìš”ì•½

| # | íŒŒì¼ | ë²„ê·¸ | ì‹¬ê°ë„ | ìƒíƒœ |
|----|------|------|--------|------|
| 1 | requirements.txt | ì˜¤íƒ€: requeststreamlit | ğŸ”´ ë†’ìŒ | âœ… ìˆ˜ì •ë¨ |
| 2 | auth.py | can_add_watchlist() ë¯¸ì‘ë™ | ğŸ”´ ë†’ìŒ | âœ… ìˆ˜ì •ë¨ |
| 3 | config.yaml | ë¹„ë°€ë²ˆí˜¸ ë…¸ì¶œ | ğŸŸ  ì¤‘ê°„ | âœ… ìˆ˜ì •ë¨ |
| 4 | browser_scraper.py | í•¨ìˆ˜ ë¯¸ì •ì˜ | ğŸŸ  ì¤‘ê°„ | âœ… ìˆ˜ì •ë¨ |
| 5 | worker/tasks.py | ì•Œë¦¼ ê¸°ëŠ¥ ì—†ìŒ | ğŸŸ  ì¤‘ê°„ | âœ… êµ¬í˜„ë¨ |
| 6 | main.py | í•„í„°ë§ ë¯¸ì ìš© | ğŸŸ  ì¤‘ê°„ | âœ… ìˆ˜ì •ë¨ |
| 7 | scraper.py | Rate limiting ì•½í•¨ | ğŸŸ¡ ë‚®ìŒ | âœ… ê°œì„ ë¨ |

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

### í•„ìš”í•œ ì¶”ê°€ ì‘ì—… (ìš°ì„ ìˆœìœ„ ìˆœ)
1. **í™˜ê²½ ì„¤ì •**
   ```bash
   # .env íŒŒì¼ ìƒì„±
   cp .env.example .env
   # ê°œì¸ ì„¤ì • ì…ë ¥
   nano .env
   ```

2. **ì˜ì¡´ì„± ì„¤ì¹˜**
   ```bash
   pip install -r requirements.txt
   playwright install  # Playwright ë¸Œë¼ìš°ì € ì„¤ì¹˜ í•„ìˆ˜!
   ```

3. **ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”**
   ```bash
   python3 -c "from src.database import RealEstateDB; RealEstateDB()"
   ```

4. **Celery ì„¤ì •** (ì„ íƒ)
   ```bash
   # Redis ì„¤ì¹˜ (í™ˆë¸Œë£¨)
   brew install redis
   redis-server  # Redis ì‹¤í–‰
   
   # Celery ì›Œì»¤ ì‹¤í–‰
   celery -A celery_config worker --loglevel=info
   
   # Celery Beat ì‹¤í–‰ (ìŠ¤ì¼€ì¤„ëŸ¬)
   celery -A celery_config beat --loglevel=info
   ```

5. **ì• í”Œë¦¬ì¼€ì´ì…˜ í…ŒìŠ¤íŠ¸**
   ```bash
   # Streamlit ì•± ì‹¤í–‰
   streamlit run app.py
   
   # ë˜ëŠ” ë°ì´í„° ìˆ˜ì§‘
   python3 main.py
   ```

---

## ğŸ“ ë³´ì•ˆ ì²´í¬ë¦¬ìŠ¤íŠ¸

- âœ… `.env` íŒŒì¼ `.gitignore`ì— ì¶”ê°€ë¨
- âœ… `config.yaml`ì—ì„œ ë¯¼ê°í•œ ì •ë³´ ì œê±°ë¨
- âœ… ë¹„ë°€ë²ˆí˜¸ëŠ” í™˜ê²½ ë³€ìˆ˜ë¡œ ê´€ë¦¬
- âœ… `.gitignore`ì—ì„œ `*.db` íŒŒì¼ ì œì™¸ë¨

---

## ğŸ¯ í…ŒìŠ¤íŠ¸ ì™„ë£Œ

```bash
# ëª¨ë“  ìˆ˜ì • íŒŒì¼ ë¬¸ë²• í™•ì¸ âœ…
python3 -m py_compile src/auth.py src/browser_scraper.py worker/tasks.py src/scraper.py main.py
```

---

## ğŸ’¡ ì¶”ê°€ ê°œì„  ì‚¬í•­ (í–¥í›„ ì‘ì—…)

### ë‚®ì€ ì‹¬ê°ë„ í•­ëª©ë“¤
- [ ] ë°ì´í„°ë² ì´ìŠ¤ ë³µí•© ì¸ë±ìŠ¤ ì¶”ê°€ (ì„±ëŠ¥ ê°œì„ )
- [ ] ì‚¬ìš©ì êµ¬ë… í”Œëœ ê¸°ëŠ¥ ì™„ì„±
- [ ] ì´ë©”ì¼ í…œí”Œë¦¿ HTML ê°œì„ 
- [ ] ë¡œê·¸ ì‹œìŠ¤í…œ ê³ ë„í™”
- [ ] í…ŒìŠ¤íŠ¸ ì½”ë“œ ì‘ì„±

---

**ì‘ì„±ì**: AI Assistant  
**ì™„ë£Œ ì‹œê°„**: 2026ë…„ 1ì›” 16ì¼  
**ìƒíƒœ**: âœ… ëª¨ë“  ì‹¬ê°ë„ ë†’ì€ ë²„ê·¸ ìˆ˜ì • ì™„ë£Œ
